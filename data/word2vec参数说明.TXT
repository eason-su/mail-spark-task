
time ./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -binary 1 -iter 15

time 求训练时间
./word2vec 训练执行文件
-train 训练数据 
-vectors.bin 输出词向量文件
-output 结果输入文件，即每个词的向量 
-cbow 是否使用cbow模型，0表示使用skip-gram模型，1表示使用cbow模型，默认情况下是skip-gram模型，cbow模型快一些，skip-gram模型效果好一些 
-size 表示输出的词向量维数 
-window 为训练的窗口大小，8表示每个词考虑前8个词与后8个词（实际代码中还有一个随机选窗口的过程，窗口大小<=5) 
-negative 表示是否使用NEG方，0表示不使用，其它的值目前还不是很清楚 
-hs 是否使用HS方法，0表示不使用，1表示使用 
-sample 表示 采样的阈值，如果一个词在训练样本中出现的频率越大，那么就越会被采样 
-binary 表示输出的结果文件是否采用二进制存储，0表示不使用（即普通的文本存储，可以打开查看），1表示使用，即vectors.bin的存储类型


Mikolov 关于超参数的建议如下：
1. 模型架构：Skip-gram 更慢一些，但是对低频词效果更好；对应的 CBOW 则速度更快一些。 
2. 训练算法：层次 softmax 对低频词效果更好；对应的 negative sampling 对 高频词效果更好，向量维度较低时效果更好。 
3. 高频词亚采样：对大数据集合可以同时提高精度和速度，sample 的取值 在 1e-3 到 1e-5 之间效果最佳。 
4. 词向量的维度：一般维度越高越好，但并不总是这样。 
5. 窗口大小：Skip-gram 一般 10 左右，CBOW 一般 5 左右。 